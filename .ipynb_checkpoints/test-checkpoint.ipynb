{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "150e86e9-0a09-4016-953f-0173eecd39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TCSS456'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/ljeong072/TCSS456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c321ef5-7ec7-4cc5-b7c8-66ec811eaf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md', '.git']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set directory into cloned repo and open the files to check.\n",
    "os.chdir(\"TCSS456\")  \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2f16f0f-0f80-4195-95b1-1f38223dbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mDev\u001b[m\n",
      "  main\u001b[m\n",
      "  \u001b[31mremotes/origin/Dev\u001b[m\n",
      "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
      "  \u001b[31mremotes/origin/mahri\u001b[m\n",
      "  \u001b[31mremotes/origin/main\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# See available branches\n",
    "!git branch -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed00898e-e388-40ba-843d-eab19ff1047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'Dev'\n",
      "Your branch is up to date with 'origin/Dev'.\n"
     ]
    }
   ],
   "source": [
    "# Checkout Dev branch\n",
    "!git checkout Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53a1e49c-7d9d-4d11-acf5-51cbf3f84570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "# Status of branch\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baf1621b-c014-4b73-90dc-1f81e2e87a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Push to Github (Change the message and check that this is the correct branch\n",
    "!git add .\n",
    "!git commit -m \"First commit\"\n",
    "!git push\n",
    "!git push origin Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89469c62-d5da-4324-922c-a75c28b15ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Prerequisites\n",
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1961e533-f17d-4b40-b049-1c2d29639442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Prompt', 'Formal specification', 'Max per-meal sodium (mg)', 'Max daily saturated fat (g)', 'Max daily calories (kcal)', 'Min daily fiber (g)', 'Min daily servings of vegetables', 'Min daily servings of fruit', 'Max daily percentage carbohydrates', 'Min daily percentage carbohydrates', 'Max daily percentage fat', 'Min daily percentage fat', 'Max daily percentage protein', 'Min daily percentage protein', 'Max daily servings of whole grains', 'Min daily servings of whole grains', 'Max servings of sweets per week', 'Max servings of red meat per week', 'Min servings of nuts, seeds, legumes per week', 'Max servings of low-fat dairy per week', 'Min servings of low-fat dairy per week', 'Max servings of fish per week', 'Min servings of fish per week', 'Dietary preference', 'Flavor preference', 'Cooking preference', 'Cuisine preference'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\n",
    "dataset = load_dataset(\"thomasat/diet-planning\")\n",
    "# Change to this one dataset\n",
    "#from datasets import load_dataset\n",
    "\n",
    "# Split the train split into 80%/20% test\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Access the new splits\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "#dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# Load the dataset\n",
    "#dataset[\"train\"] = dataset[\"train\"].select(range(100))\n",
    "#dataset[\"validation\"] = dataset[\"validation\"].select(range(10))\n",
    "\n",
    "# Test split will not be used\n",
    "#dataset[\"test\"] = dataset[\"test\"].select(range(0))\n",
    "\n",
    "\n",
    "#print(dataset)\n",
    "#print(dataset['train'][10])\n",
    "#print(dataset['validation'][5])\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd367ce9-5691-4491-9e5f-6013dc5162ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Formal specification', 'Max per-meal sodium (mg)', 'Max daily saturated fat (g)', 'Max daily calories (kcal)', 'Min daily fiber (g)', 'Min daily servings of vegetables', 'Min daily servings of fruit', 'Max daily percentage carbohydrates', 'Min daily percentage carbohydrates', 'Max daily percentage fat', 'Min daily percentage fat', 'Max daily percentage protein', 'Min daily percentage protein', 'Max daily servings of whole grains', 'Min daily servings of whole grains', 'Max servings of sweets per week', 'Max servings of red meat per week', 'Min servings of nuts, seeds, legumes per week', 'Max servings of low-fat dairy per week', 'Min servings of low-fat dairy per week', 'Max servings of fish per week', 'Min servings of fish per week', 'Dietary preference', 'Flavor preference', 'Cooking preference', 'Cuisine preference', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 Step 2: Tokenize the Text\n",
    "# Note: We have used distilbert-base-uncased tokenizer in Tutorial_1\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load the model (GPT-2 Small)\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "def tokenize_function(example):\n",
    "  return tokenizer(example[\"Prompt\"])\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"Prompt\"])\n",
    "tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ecd3598-6798-4bf3-b6ba-b1416ec87c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badbe7d5266143fbb914f29df426e70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 26 named input_ids expected length 1000 but got length 993",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m     result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m---> 17\u001b[0m lm_dataset \u001b[38;5;241m=\u001b[39m tokenized_dataset\u001b[38;5;241m.\u001b[39mmap(group_texts, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m lm_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/dataset_dict.py:902\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    901\u001b[0m     {\n\u001b[0;32m--> 902\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    903\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    904\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    905\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    906\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    907\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    908\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    909\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    910\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    911\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    912\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    913\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    914\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    915\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    916\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    917\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    918\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    919\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    920\u001b[0m         )\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    922\u001b[0m     }\n\u001b[1;32m    923\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py:3534\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3532\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[1;32m   3533\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3534\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[1;32m   3535\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[1;32m   3536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/datasets/arrow_writer.py:608\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    606\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[1;32m    607\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[0;32m--> 608\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/table.pxi:4642\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/table.pxi:3922\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 26 named input_ids expected length 1000 but got length 993"
     ]
    }
   ],
   "source": [
    "#1.4 Step 3: Group Tokens for Language Modeling\n",
    "block_size = 236\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = sum(examples['input_ids'], [])\n",
    "    concatenated_attention_mask = sum(examples['attention_mask'], [])\n",
    "    total_length = (len(concatenated) // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        'input_ids': [concatenated[i:i + block_size] for i in range(0, total_length, block_size)],\n",
    "        'attention_mask': [concatenated_attention_mask[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "lm_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c270976-bb3b-426e-970d-272aa55643d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Step 4: Load a Pretrained Model for Language Modeling\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # Note: we are using gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca6937-1355-41f5-bbe4-e22ed868940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model and selects a GPU if possible\n",
    "# Some hyperparameters were changed as Google Colab's TPU and GPU\n",
    "# had expired, so the CPU was utilized with parameters adjusted to\n",
    "# allow it to train in a reasonable amount of time.\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 1. Leverage mixed precision training\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./lm_checkpoints\",\n",
    "  # 2. Aligned evaluation and saving strategy\n",
    "    \n",
    "  eval_strategy=\"steps\", # Evaluate at specific steps\n",
    "  eval_steps=100, # Evaluate every 100 steps\n",
    "\n",
    "  save_strategy=\"steps\", # Does not allow equality to \"epoch\"\n",
    "\n",
    "  #save_steps=100,\n",
    "  save_steps=1000, # Save every 1000 steps\n",
    "  save_total_limit=2, # Keep only the 2 most recent checkpoints\n",
    "\n",
    "  # 3. Increase learning rate and use warmup\n",
    "  learning_rate=5e-5, # Higher learning rate\n",
    "  warmup_ratio=0.1, # Warm up for first 10% of training\n",
    "\n",
    "  # 4. Increase batch size\n",
    "  per_device_train_batch_size=16, # Increase if your GPU has enough memory\n",
    "  per_device_eval_batch_size=16,\n",
    "  gradient_accumulation_steps=2, # Simulate larger batch sizes\n",
    "\n",
    "  # 5. Enable fp16 training (mixed precision)\n",
    "  fp16=True, # Enable mixed precision training\n",
    "\n",
    "  # 6. Early stopping configuration\n",
    "  load_best_model_at_end=True, # Load the best model when training ends\n",
    "  metric_for_best_model=\"loss\", # Use evaluation loss as the metric to track\n",
    "  greater_is_better=False, # Lower loss is better\n",
    "\n",
    "  # 7. Other optimizations\n",
    "  weight_decay=0.01,\n",
    "  logging_steps=50, # Less frequent logging\n",
    "  report_to=\"none\",\n",
    "\n",
    "  # 8. Enable data parallelism if multiple GPUs are available\n",
    "  dataloader_num_workers= 2, # Use multiple CPU cores for data loading\n",
    "  # 9. Set number of epochs\n",
    "  num_train_epochs=1, # Maintain the original epochs setting\n",
    ")\n",
    "# 9. Initialize the Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=lm_dataset[\"train\"],\n",
    "  eval_dataset=lm_dataset[\"test\"]\n",
    ")\n",
    "# 10. Optional: Use early stopping\n",
    "early_stopping_callback = transformers.EarlyStoppingCallback(\n",
    "  early_stopping_patience=3,\n",
    "  early_stopping_threshold=0.01\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f2223c-3ace-40f5-a282-3ecd299e070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output1 = generator(\"Raging wildfires shook California as\", max_length=40, num_return_sequences=1)\n",
    "output2 = generator(\"Harry Potter is a book\", max_length = 40, num_return_sequences=1)\n",
    "output3 = generator(\"The Republic of Korea\", max_length=40, num_return_sequences=1)\n",
    "\n",
    "# 3 unique sentences for generation testing\n",
    "print(output1)\n",
    "print(output2)\n",
    "print(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d76bbdd-ade7-4623-aaec-d8089496d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae766b2-50cd-4e80-b739-b0b581c4ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330154e5-e20b-4ab7-9307-f327e686ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d10b91-8ca5-492d-b53c-dadbf9dfd876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43367e12-a44f-4a68-abdd-570ae2c175be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
