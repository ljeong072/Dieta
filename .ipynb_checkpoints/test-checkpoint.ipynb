{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "150e86e9-0a09-4016-953f-0173eecd39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TCSS456'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/ljeong072/TCSS456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c321ef5-7ec7-4cc5-b7c8-66ec811eaf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md', '.git']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set directory into cloned repo and open the files to check.\n",
    "os.chdir(\"TCSS456\")  \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2f16f0f-0f80-4195-95b1-1f38223dbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mDev\u001b[m\n",
      "  main\u001b[m\n",
      "  \u001b[31mremotes/origin/Dev\u001b[m\n",
      "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
      "  \u001b[31mremotes/origin/mahri\u001b[m\n",
      "  \u001b[31mremotes/origin/main\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# See available branches\n",
    "!git branch -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed00898e-e388-40ba-843d-eab19ff1047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'Dev'\n",
      "Your branch is up to date with 'origin/Dev'.\n"
     ]
    }
   ],
   "source": [
    "# Checkout Dev branch\n",
    "!git checkout Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53a1e49c-7d9d-4d11-acf5-51cbf3f84570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "# Status of branch\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baf1621b-c014-4b73-90dc-1f81e2e87a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Push to Github (Change the message and check that this is the correct branch\n",
    "!git add .\n",
    "!git commit -m \"First commit\"\n",
    "!git push\n",
    "!git push origin Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89469c62-d5da-4324-922c-a75c28b15ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Prerequisites\n",
    "!pip install transformers datasets pandas scikit-learn torch torchvision torchaudio ipywidgets jupyterlab-widgets 'accelerate>=0.26.0' --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9910add2-99e6-4883-8960-d5f34945e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\tuand\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y --quiet\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
    "!pip install transformers datasets pandas scikit-learn ipywidgets accelerate>=0.26.0 gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6a902-f7bf-46eb-8121-e235e35af59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import joblib\n",
    "import gradio as gr\n",
    "from datasets import concatenate_datasets, load_dataset, Dataset, DatasetDict\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "# new imports \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1961e533-f17d-4b40-b049-1c2d29639442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = load_dataset(\"Tom158/Nutritional-LLama\")\n",
    "# Additional dataset\n",
    "dataset2 = load_dataset(\"sridhar52/Augmented_Meal_Planner_data\")\n",
    "\n",
    "# Rename columns\n",
    "dataset2 = dataset2.rename_column('input', 'User')\n",
    "dataset2 = dataset2.rename_column('output', 'Nutritionist')\n",
    "\n",
    "# Concatenate datasets\n",
    "concat_dataset = concatenate_datasets([dataset1['train'], dataset2['train']])\n",
    "\n",
    "dataset = DatasetDict({'train': concat_dataset})\n",
    "\n",
    "# Split the train split into 90% train, 10% val\n",
    "split_dataset = dataset['train'].train_test_split(test_size = 0.1, seed = 42)\n",
    "print(split_dataset)\n",
    "\n",
    "# Possible issue due to N/A values on combined datasets and also N/A values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd367ce9-5691-4491-9e5f-6013dc5162ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Step 2: Tokenize the Text\n",
    "# Note: We have used distilbert-base-uncased tokenizer in Tutorial_1\n",
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model (GPT-2 Small)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def format_qa(example):\n",
    "    return {\n",
    "        \"prompt\": f\"Q: {example[\"User\"]}\\nA: {example[\"Nutritionist\"]}\"\n",
    "    }\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        # padding = \"max_length\",\n",
    "        # truncation = True,\n",
    "        # max_length = 128,\n",
    "    )\n",
    "\n",
    "split_dataset = split_dataset.map(format_qa)\n",
    "print(split_dataset[\"train\"][0][\"prompt\"])\n",
    "tokenized_dataset = split_dataset.map(tokenize_function, batched = True, remove_columns = [\"System\", \"User\", \"Nutritionist\", \"text\", \"prompt\"])\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecd3598-6798-4bf3-b6ba-b1416ec87c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1.4 Step 3: Group Tokens for Language Modeling\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = sum(examples['input_ids'], [])\n",
    "    concatenated_attention_mask = sum(examples['attention_mask'], [])\n",
    "    \n",
    "    total_length = (len(concatenated) // block_size) * block_size\n",
    "    result = {\n",
    "        'input_ids': [concatenated[i:i + block_size] for i in range(0, total_length, block_size)],\n",
    "        'attention_mask': [concatenated_attention_mask[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched = True)\n",
    "lm_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c270976-bb3b-426e-970d-272aa55643d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Step 4: Load a Pretrained Model for Language Modeling\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\") # Note: we are using gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1a3eb-9ba3-4ca6-9e36-077a06d7e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model and selects a GPU if possible\n",
    "# Some hyperparameters were changed as Google Colab's TPU and GPU\n",
    "# had expired, so the CPU was utilized with parameters adjusted to\n",
    "# allow it to train in a reasonable amount of time.\n",
    "\n",
    "# # 1. Leverage mixed precision training\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir = \"./lm_checkpoints\",\n",
    "    \n",
    "#     # 2. Aligned evaluation and saving strategy\n",
    "#     eval_strategy = \"steps\", # Evaluate at specific steps\n",
    "#     eval_steps = 100, # Evaluate every 100 steps\n",
    "#     save_strategy = \"steps\", # Does not allow equality to \"epoch\"\n",
    "#     save_steps = 100, # Save every 100 steps\n",
    "#     save_total_limit = 2, # Keep only the 2 most recent checkpoints\n",
    "\n",
    "#     # 3. Increase learning rate and use warmup\n",
    "#     learning_rate = 5e-5, # Higher learning rate\n",
    "#     warmup_ratio = 0.1, # Warm up for first 10% of training\n",
    "\n",
    "#     # 4. Increase batch size\n",
    "#     per_device_train_batch_size = 16, # Increase if your GPU has enough memory\n",
    "#     # per_device_eval_batch_size = 16,\n",
    "#     gradient_accumulation_steps = 2, # Simulate larger batch sizes\n",
    "\n",
    "#     # 5. Enable fp16 training (mixed precision)\n",
    "#     fp16 = True, # Enable mixed precision training\n",
    "\n",
    "#     # 6. Early stopping configuration\n",
    "#     load_best_model_at_end = True, # Load the best model when training ends\n",
    "#     metric_for_best_model = \"loss\", # Use evaluation loss as the metric to track\n",
    "#     greater_is_better = False, # Lower loss is better\n",
    "\n",
    "#     # 7. Other optimizations\n",
    "#     weight_decay = 0.01,\n",
    "#     logging_steps = 50, # Less frequent logging\n",
    "#     report_to = \"none\",\n",
    "\n",
    "#     # 8. Enable data parallelism if multiple GPUs are available\n",
    "#     dataloader_num_workers =  4, # Use multiple CPU cores for data loading\n",
    "\n",
    "#     # 9. Set number of epochs\n",
    "#     num_train_epochs = 1, # Maintain the original epochs setting\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./qa-gpt2\",\n",
    "    overwrite_output_dir = True,\n",
    "    \n",
    "    num_train_epochs = 3,\n",
    "    \n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "\n",
    "    #test\n",
    "\n",
    "    learning_rate = 5e-5, # Higher learning rate\n",
    "    warmup_ratio = 0.1, # Warm up for first 10% of training\n",
    "    eval_strategy=\"steps\", # Evaluate at specific steps\n",
    "    save_strategy=\"steps\", # Save at the same frequency as evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    save_steps = 100,\n",
    "    eval_steps = 100,\n",
    "    save_total_limit = 2,\n",
    "\n",
    "    # test \n",
    "    load_best_model_at_end = True, # Load the best model when training ends\n",
    "    metric_for_best_model = \"loss\", # Use evaluation loss as the metric to track\n",
    "    greater_is_better = False, # Lower loss is better\n",
    "      \n",
    "    weight_decay = 0.01,\n",
    "    \n",
    "    prediction_loss_only = True,\n",
    "    logging_dir = \"./logs\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer, mlm = False\n",
    ")\n",
    "\n",
    "# 9. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = lm_dataset[\"train\"],\n",
    "    eval_dataset = lm_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "# 10. Optional: Use early stopping\n",
    "early_stopping_callback = transformers.EarlyStoppingCallback(\n",
    "     early_stopping_patience = 3,\n",
    "     early_stopping_threshold = 0.01\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"my_model\")\n",
    "tokenizer.save_pretrained(\"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34745c-f9e5-4d84-a0dd-6f486c8f74a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, \"model.pkl\")\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c67c2-46b1-4d32-acf4-72fa97c11ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"model.pkl\")\n",
    "tokenizer = joblib.load(\"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ed278-cdcd-477b-bcfe-50708f135a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "output1 = qa_pipeline(\"Q: Can you recommend me a healthy diet?\\nA:\", max_new_tokens = 50)\n",
    "# output2 = qa_pipeline(\"Q: A diet consisting of 100 mg of salt, 200 mg of fat, and\\nA:\", max_new_tokens = 50)\n",
    "output3 = qa_pipeline(\"Q: I prefer Chinese cuisine, and I want to lower my sodium. What can you recommend me?\\nA:\", max_new_tokens = 50)\n",
    "# output4 = qa_pipeline(\"Q: I am a pescatarian and enjoy spicy food. What should I eat?\\nA:\", max_new_tokens = 50)\n",
    "# output5 = qa_pipeline(\"Q: I need to consume more fruit and carbohydrates a day. What should I do?\\nA:\", max_new_tokens = 50)\n",
    "# output6 = qa_pipeline(\"Q: My maximum daily calories is 1,200 so I should consume \\nA:\", max_new_tokens = 50)\n",
    "# output7 = qa_pipeline(\"Q: I am a vegetarian and I enjoy food that is sweet and salty. Can you recommend me a diet?\\nA:\", max_new_tokens = 50)\n",
    "output8 = qa_pipeline(\"Q: I've been trying to lose some weight and get in shape, but I love ground lean as part of my breakfast routine. Is it okay to include this in my diet considering I'm 43 and 109kg with overweight?\\nA:\", max_new_tokens = 50)\n",
    "\n",
    "# 3 unique sentences for generation testing\n",
    "print(output1[0][\"generated_text\"])\n",
    "# print(output2[0][\"generated_text\"])\n",
    "print(output3[0][\"generated_text\"])\n",
    "# print(output4[0][\"generated_text\"])\n",
    "# print(output5[0][\"generated_text\"])\n",
    "# print(output6[0][\"generated_text\"])\n",
    "# print(output7[0][\"generated_text\"])\n",
    "print(output8[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07939dc-a461-4b9c-8d95-b410174d4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generation function\n",
    "def generate_answer(prompt):\n",
    "    try:\n",
    "        result = qa_pipeline(prompt, max_new_tokens = 100, do_sample = True, temperature = 0.9)\n",
    "        result = result[0]['generated_text']\n",
    "\n",
    "        # Extract answer after \"A:\"\n",
    "        if \"A:\" in result:\n",
    "            result = result.split(\"A:\", 1)[1].strip()\n",
    "        else:\n",
    "            result = result.strip()  # fallback if format breaks\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {str(e)}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_answer,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type a question or prompt...\", label=\"Prompt\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"GPT-2 Q&A Generator\",\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7762a-58a6-4f2d-940c-03e83ccae706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20beac9-0a59-404d-b766-ca1d0048def1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
