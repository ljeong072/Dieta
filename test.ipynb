{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "150e86e9-0a09-4016-953f-0173eecd39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TCSS456'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/ljeong072/TCSS456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c321ef5-7ec7-4cc5-b7c8-66ec811eaf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md', '.git']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set directory into cloned repo and open the files to check.\n",
    "os.chdir(\"TCSS456\")  \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2f16f0f-0f80-4195-95b1-1f38223dbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mDev\u001b[m\n",
      "  main\u001b[m\n",
      "  \u001b[31mremotes/origin/Dev\u001b[m\n",
      "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
      "  \u001b[31mremotes/origin/mahri\u001b[m\n",
      "  \u001b[31mremotes/origin/main\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# See available branches\n",
    "!git branch -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed00898e-e388-40ba-843d-eab19ff1047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'Dev'\n",
      "Your branch is up to date with 'origin/Dev'.\n"
     ]
    }
   ],
   "source": [
    "# Checkout Dev branch\n",
    "!git checkout Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53a1e49c-7d9d-4d11-acf5-51cbf3f84570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "# Status of branch\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baf1621b-c014-4b73-90dc-1f81e2e87a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Push to Github (Change the message and check that this is the correct branch\n",
    "!git add .\n",
    "!git commit -m \"First commit\"\n",
    "!git push\n",
    "!git push origin Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89469c62-d5da-4324-922c-a75c28b15ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Prerequisites\n",
    "!pip install transformers datasets pandas scikit-learn torch torchvision torchaudio ipywidgets accelerate>=0.26.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf6a902-f7bf-46eb-8121-e235e35af59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import joblib\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961e533-f17d-4b40-b049-1c2d29639442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Prompt', 'Formal specification', 'Max per-meal sodium (mg)', 'Max daily saturated fat (g)', 'Max daily calories (kcal)', 'Min daily fiber (g)', 'Min daily servings of vegetables', 'Min daily servings of fruit', 'Max daily percentage carbohydrates', 'Min daily percentage carbohydrates', 'Max daily percentage fat', 'Min daily percentage fat', 'Max daily percentage protein', 'Min daily percentage protein', 'Max daily servings of whole grains', 'Min daily servings of whole grains', 'Max servings of sweets per week', 'Max servings of red meat per week', 'Min servings of nuts, seeds, legumes per week', 'Max servings of low-fat dairy per week', 'Min servings of low-fat dairy per week', 'Max servings of fish per week', 'Min servings of fish per week', 'Dietary preference', 'Flavor preference', 'Cooking preference', 'Cuisine preference'],\n",
      "        num_rows: 8000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Prompt', 'Formal specification', 'Max per-meal sodium (mg)', 'Max daily saturated fat (g)', 'Max daily calories (kcal)', 'Min daily fiber (g)', 'Min daily servings of vegetables', 'Min daily servings of fruit', 'Max daily percentage carbohydrates', 'Min daily percentage carbohydrates', 'Max daily percentage fat', 'Min daily percentage fat', 'Max daily percentage protein', 'Min daily percentage protein', 'Max daily servings of whole grains', 'Min daily servings of whole grains', 'Max servings of sweets per week', 'Max servings of red meat per week', 'Min servings of nuts, seeds, legumes per week', 'Max servings of low-fat dairy per week', 'Min servings of low-fat dairy per week', 'Max servings of fish per week', 'Min servings of fish per week', 'Dietary preference', 'Flavor preference', 'Cooking preference', 'Cuisine preference'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "\n",
    "dataset = load_dataset(\"thomasat/diet-planning\")\n",
    "\n",
    "# Split the train split into 80% train, 20% val\n",
    "split_dataset = dataset['train'].train_test_split(test_size = 0.2, seed = 42)\n",
    "\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd367ce9-5691-4491-9e5f-6013dc5162ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3 Step 2: Tokenize the Text\n",
    "# Note: We have used distilbert-base-uncased tokenizer in Tutorial_1\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model (GPT-2 Small)\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(example):\n",
    "  return tokenizer(example[\"Prompt\"])\n",
    "\n",
    "tokenized_dataset = split_dataset.map(tokenize_function, batched = True, remove_columns = [\"Prompt\", \"Formal specification\", \"Max per-meal sodium (mg)\", \"Max daily saturated fat (g)\", \"Max daily calories (kcal)\", \"Min daily fiber (g)\", \"Min daily servings of vegetables\", \"Min daily servings of fruit\", \"Max daily percentage carbohydrates\", \"Min daily percentage carbohydrates\", \"Max daily percentage fat\", \"Min daily percentage fat\", \"Max daily percentage protein\", \"Min daily percentage protein\", \"Max daily servings of whole grains\", \"Min daily servings of whole grains\", \"Max servings of sweets per week\", \"Max servings of red meat per week\", \"Min servings of nuts, seeds, legumes per week\", \"Max servings of low-fat dairy per week\", \"Min servings of low-fat dairy per week\", \"Max servings of fish per week\", \"Min servings of fish per week\", \"Dietary preference\", \"Flavor preference\", \"Cooking preference\", \"Cuisine preference\"])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ecd3598-6798-4bf3-b6ba-b1416ec87c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1639,\n",
       "  389,\n",
       "  281,\n",
       "  5887,\n",
       "  5496,\n",
       "  6749,\n",
       "  13,\n",
       "  4222,\n",
       "  7716,\n",
       "  257,\n",
       "  5496,\n",
       "  1410,\n",
       "  329,\n",
       "  530,\n",
       "  1285,\n",
       "  11,\n",
       "  543,\n",
       "  12991,\n",
       "  4340,\n",
       "  262,\n",
       "  2836,\n",
       "  447,\n",
       "  247,\n",
       "  82,\n",
       "  14676,\n",
       "  981,\n",
       "  635,\n",
       "  10941,\n",
       "  9573,\n",
       "  13,\n",
       "  4222,\n",
       "  1826,\n",
       "  262,\n",
       "  1708,\n",
       "  17778,\n",
       "  25,\n",
       "  198,\n",
       "  9806,\n",
       "  583,\n",
       "  12,\n",
       "  28208,\n",
       "  21072,\n",
       "  286,\n",
       "  38123,\n",
       "  10527,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  24725,\n",
       "  3735,\n",
       "  286,\n",
       "  1511,\n",
       "  308,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  14653,\n",
       "  286,\n",
       "  4751,\n",
       "  11,\n",
       "  949,\n",
       "  4445,\n",
       "  13608,\n",
       "  286,\n",
       "  2608,\n",
       "  308,\n",
       "  11,\n",
       "  949,\n",
       "  718,\n",
       "  4445,\n",
       "  43096,\n",
       "  286,\n",
       "  13701,\n",
       "  11,\n",
       "  220,\n",
       "  949,\n",
       "  362,\n",
       "  4445,\n",
       "  43096,\n",
       "  286,\n",
       "  8234,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  5873,\n",
       "  32328,\n",
       "  5996,\n",
       "  11,\n",
       "  949,\n",
       "  4445,\n",
       "  5873,\n",
       "  32328,\n",
       "  4153,\n",
       "  11,\n",
       "  220,\n",
       "  3509,\n",
       "  4445,\n",
       "  5873,\n",
       "  7532,\n",
       "  1679,\n",
       "  11,\n",
       "  949,\n",
       "  4445,\n",
       "  5873,\n",
       "  7532,\n",
       "  1315,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  5873,\n",
       "  3735,\n",
       "  1542,\n",
       "  11,\n",
       "  220,\n",
       "  949,\n",
       "  4445,\n",
       "  5873,\n",
       "  3735,\n",
       "  1160,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  43096,\n",
       "  286,\n",
       "  2187,\n",
       "  21824,\n",
       "  807,\n",
       "  11],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [1639,\n",
       "  389,\n",
       "  281,\n",
       "  5887,\n",
       "  5496,\n",
       "  6749,\n",
       "  13,\n",
       "  4222,\n",
       "  7716,\n",
       "  257,\n",
       "  5496,\n",
       "  1410,\n",
       "  329,\n",
       "  530,\n",
       "  1285,\n",
       "  11,\n",
       "  543,\n",
       "  12991,\n",
       "  4340,\n",
       "  262,\n",
       "  2836,\n",
       "  447,\n",
       "  247,\n",
       "  82,\n",
       "  14676,\n",
       "  981,\n",
       "  635,\n",
       "  10941,\n",
       "  9573,\n",
       "  13,\n",
       "  4222,\n",
       "  1826,\n",
       "  262,\n",
       "  1708,\n",
       "  17778,\n",
       "  25,\n",
       "  198,\n",
       "  9806,\n",
       "  583,\n",
       "  12,\n",
       "  28208,\n",
       "  21072,\n",
       "  286,\n",
       "  38123,\n",
       "  10527,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  24725,\n",
       "  3735,\n",
       "  286,\n",
       "  1511,\n",
       "  308,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  14653,\n",
       "  286,\n",
       "  4751,\n",
       "  11,\n",
       "  949,\n",
       "  4445,\n",
       "  13608,\n",
       "  286,\n",
       "  2608,\n",
       "  308,\n",
       "  11,\n",
       "  949,\n",
       "  718,\n",
       "  4445,\n",
       "  43096,\n",
       "  286,\n",
       "  13701,\n",
       "  11,\n",
       "  220,\n",
       "  949,\n",
       "  362,\n",
       "  4445,\n",
       "  43096,\n",
       "  286,\n",
       "  8234,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  5873,\n",
       "  32328,\n",
       "  5996,\n",
       "  11,\n",
       "  949,\n",
       "  4445,\n",
       "  5873,\n",
       "  32328,\n",
       "  4153,\n",
       "  11,\n",
       "  220,\n",
       "  3509,\n",
       "  4445,\n",
       "  5873,\n",
       "  7532,\n",
       "  1679,\n",
       "  11,\n",
       "  949,\n",
       "  4445,\n",
       "  5873,\n",
       "  7532,\n",
       "  1315,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  5873,\n",
       "  3735,\n",
       "  1542,\n",
       "  11,\n",
       "  220,\n",
       "  949,\n",
       "  4445,\n",
       "  5873,\n",
       "  3735,\n",
       "  1160,\n",
       "  11,\n",
       "  3509,\n",
       "  4445,\n",
       "  43096,\n",
       "  286,\n",
       "  2187,\n",
       "  21824,\n",
       "  807,\n",
       "  11]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.4 Step 3: Group Tokens for Language Modeling\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = sum(examples['input_ids'], [])\n",
    "    concatenated_attention_mask = sum(examples['attention_mask'], [])\n",
    "    \n",
    "    total_length = (len(concatenated) // block_size) * block_size\n",
    "    result = {\n",
    "        'input_ids': [concatenated[i:i + block_size] for i in range(0, total_length, block_size)],\n",
    "        'attention_mask': [concatenated_attention_mask[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched = True)\n",
    "lm_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c270976-bb3b-426e-970d-272aa55643d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Step 4: Load a Pretrained Model for Language Modeling\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # Note: we are using gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aca6937-1355-41f5-bbe4-e22ed868940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='457' max='457' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [457/457 1:03:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>0.134233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.119927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.119700</td>\n",
       "      <td>0.113403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.110726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\tuand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\tuand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=457, training_loss=0.3204675093782287, metrics={'train_runtime': 3797.8549, 'train_samples_per_second': 3.854, 'train_steps_per_second': 0.12, 'total_flos': 955283668992000.0, 'train_loss': 0.3204675093782287, 'epoch': 0.9989071038251366})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trains the model and selects a GPU if possible\n",
    "# Some hyperparameters were changed as Google Colab's TPU and GPU\n",
    "# had expired, so the CPU was utilized with parameters adjusted to\n",
    "# allow it to train in a reasonable amount of time.\n",
    "\n",
    "# 1. Leverage mixed precision training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./lm_checkpoints\",\n",
    "    \n",
    "    # 2. Aligned evaluation and saving strategy\n",
    "    eval_strategy = \"steps\", # Evaluate at specific steps\n",
    "    eval_steps = 100, # Evaluate every 100 steps\n",
    "    save_strategy = \"steps\", # Does not allow equality to \"epoch\"\n",
    "    save_steps = 100, # Save every 100 steps\n",
    "    save_total_limit = 2, # Keep only the 2 most recent checkpoints\n",
    "\n",
    "    # 3. Increase learning rate and use warmup\n",
    "    learning_rate = 5e-5, # Higher learning rate\n",
    "    warmup_ratio = 0.1, # Warm up for first 10% of training\n",
    "\n",
    "    # 4. Increase batch size\n",
    "    per_device_train_batch_size = 16, # Increase if your GPU has enough memory\n",
    "    per_device_eval_batch_size = 16,\n",
    "    gradient_accumulation_steps = 2, # Simulate larger batch sizes\n",
    "\n",
    "    # 5. Enable fp16 training (mixed precision)\n",
    "    fp16 = True, # Enable mixed precision training\n",
    "\n",
    "    # 6. Early stopping configuration\n",
    "    load_best_model_at_end = True, # Load the best model when training ends\n",
    "    metric_for_best_model = \"loss\", # Use evaluation loss as the metric to track\n",
    "    greater_is_better = False, # Lower loss is better\n",
    "\n",
    "    # 7. Other optimizations\n",
    "    weight_decay = 0.01,\n",
    "    logging_steps = 50, # Less frequent logging\n",
    "    report_to = \"none\",\n",
    "\n",
    "    # 8. Enable data parallelism if multiple GPUs are available\n",
    "    dataloader_num_workers =  4, # Use multiple CPU cores for data loading\n",
    "\n",
    "    # 9. Set number of epochs\n",
    "    num_train_epochs = 1, # Maintain the original epochs setting\n",
    ")\n",
    "\n",
    "# 9. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "  model = model,\n",
    "  args = training_args,\n",
    "  train_dataset = lm_dataset[\"train\"],\n",
    "  eval_dataset = lm_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "# 10. Optional: Use early stopping\n",
    "early_stopping_callback = transformers.EarlyStoppingCallback(\n",
    "  early_stopping_patience = 3,\n",
    "  early_stopping_threshold = 0.01\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "joblib.dump(model, \"model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "457ed278-cdcd-477b-bcfe-50708f135a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A healthy diet plan for one week, which maximizes the user’s satisfaction while also maintaining diversity. Please meet the following constraints:\n",
      "max per-meal sodium of 2100 mg, max daily\n",
      "A diet consisting of 100 mg of salt, 200 mg of fat, and min of 2 servings per week,  max low-fat dairy of 3 servings per week, min of 2 servings per week, max 2 servings of fish per week,  and min of 1 per week. Diet preference:\n",
      "I prefer Chinese cuisine but must lower my sodium. A diet plan for one week, which maximizes the user’s satisfaction while also maintaining diversity. Please meet the following constraints:\n",
      "max per-meal sodium of 2300 mg, max daily saturated fat of 13 g, max daily calories of\n",
      "I am a pescatarian and enjoy spicy food.  min 4 daily servings of fruit, max daily percentage carbohydrates 55, min daily percentage carbohydrates 45,  max daily percentage protein 25, min daily percentage protein 15, max daily percentage fat 30,  min daily percentage fat 20, max daily\n",
      "I need to consume more fruit and carbohydrates a day. Please meet the following constraints:\n",
      "max per-meal sodium of 1800 mg, max daily saturated fat of 13 g, max daily calories of 2000, min daily fiber of 28 g, min 6 daily servings of vegetables,  min 3 daily servings\n",
      "My maximum daily calories is 1,200 so I should consume  max 2 servings of red meat per week, min nuts, seeds, and legumes of 5 servings per week,  max low-fat dairy of 3 servings per week, min of 2 servings per week, max 2 servings of fish\n",
      "I am a vegetarian and I enjoy food that is sweet and salty. Cooking preference: Minimal cooking prep Cuisine preference: French.You are an expert dietician. Please generate a diet plan for one week, which maximizes the user’s satisfaction while also maintaining diversity. Please meet the\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(\"model.pkl\")\n",
    "generator = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "output1 = generator(\"A healthy diet\", max_length = 40, num_return_sequences = 1)\n",
    "output2 = generator(\"A diet consisting of 100 mg of salt, 200 mg of fat, and\", max_length = 60, num_return_sequences = 1)\n",
    "output3 = generator(\"I prefer Chinese cuisine but must lower my sodium. A diet\", max_length = 60, num_return_sequences = 1)\n",
    "output4 = generator(\"I am a pescatarian and enjoy spicy food. \", max_length = 60, num_return_sequences = 1)\n",
    "output5 = generator(\"I need to consume more fruit and carbohydrates a day.\", max_length = 60, num_return_sequences = 1)\n",
    "output6 = generator(\"My maximum daily calories is 1,200 so I should consume \", max_length = 60, num_return_sequences = 1)\n",
    "output7 = generator(\"I am a vegetarian and I enjoy food that is sweet and salty.\", max_length = 60, num_return_sequences = 1)\n",
    "\n",
    "# 3 unique sentences for generation testing\n",
    "print(output1[0][\"generated_text\"])\n",
    "print(output2[0][\"generated_text\"])\n",
    "print(output3[0][\"generated_text\"])\n",
    "print(output4[0][\"generated_text\"])\n",
    "print(output5[0][\"generated_text\"])\n",
    "print(output6[0][\"generated_text\"])\n",
    "print(output7[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4746bae-ca9d-49bd-9788-50e6d3f06add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
