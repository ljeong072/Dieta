{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "150e86e9-0a09-4016-953f-0173eecd39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TCSS456'...\n",
      "remote: Enumerating objects: 8, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 8 (delta 0), reused 5 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/ljeong072/TCSS456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c321ef5-7ec7-4cc5-b7c8-66ec811eaf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.md', '.git']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set directory into cloned repo and open the files to check.\n",
    "os.chdir(\"TCSS456\")  \n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2f16f0f-0f80-4195-95b1-1f38223dbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mDev\u001b[m\n",
      "  main\u001b[m\n",
      "  \u001b[31mremotes/origin/Dev\u001b[m\n",
      "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
      "  \u001b[31mremotes/origin/mahri\u001b[m\n",
      "  \u001b[31mremotes/origin/main\u001b[m\n"
     ]
    }
   ],
   "source": [
    "# See available branches\n",
    "!git branch -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed00898e-e388-40ba-843d-eab19ff1047e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already on 'Dev'\n",
      "Your branch is up to date with 'origin/Dev'.\n"
     ]
    }
   ],
   "source": [
    "# Checkout Dev branch\n",
    "!git checkout Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "53a1e49c-7d9d-4d11-acf5-51cbf3f84570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    }
   ],
   "source": [
    "# Status of branch\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baf1621b-c014-4b73-90dc-1f81e2e87a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch Dev\n",
      "Your branch is up to date with 'origin/Dev'.\n",
      "\n",
      "nothing to commit, working tree clean\n",
      "Everything up-to-date\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "# Push to Github (Change the message and check that this is the correct branch\n",
    "!git add .\n",
    "!git commit -m \"First commit\"\n",
    "!git push\n",
    "!git push origin Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89469c62-d5da-4324-922c-a75c28b15ae1",
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'accelerate\": Expected package name at the start of dependency specifier\n",
      "    'accelerate\n",
      "    ^\n"
     ]
    }
   ],
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
   "source": [
    "# 1.1 Prerequisites\n",
    "!pip install transformers datasets pandas scikit-learn torch torchvision torchaudio ipywidgets jupyterlab-widgets 'accelerate>=0.26.0' --quiet"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "9910add2-99e6-4883-8960-d5f34945e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\tuand\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
=======
   "execution_count": 2,
   "id": "9910add2-99e6-4883-8960-d5f34945e745",
   "metadata": {},
   "outputs": [],
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
   "source": [
    "!pip uninstall torch torchvision torchaudio -y --quiet\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet\n",
    "!pip install transformers datasets pandas scikit-learn ipywidgets accelerate>=0.26.0 gradio --quiet"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   "id": "bdf6a902-f7bf-46eb-8121-e235e35af59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import joblib\n",
    "import gradio as gr\n",
<<<<<<< HEAD
    "from datasets import concatenate_datasets, load_dataset, Dataset, DatasetDict\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
=======
<<<<<<< HEAD
    "from datasets import load_dataset, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline"
=======
    "from datasets import concatenate_datasets, load_dataset, Dataset, DatasetDict\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
>>>>>>> Dev
    "from transformers import pipeline\n",
    "\n",
    "# new imports \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
<<<<<<< HEAD
=======
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "1961e533-f17d-4b40-b049-1c2d29639442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = load_dataset(\"Tom158/Nutritional-LLama\")\n",
    "# Additional dataset\n",
    "dataset2 = load_dataset(\"sridhar52/Augmented_Meal_Planner_data\")\n",
    "\n",
    "# Rename columns\n",
    "dataset2 = dataset2.rename_column('input', 'User')\n",
    "dataset2 = dataset2.rename_column('output', 'Nutritionist')\n",
    "\n",
    "# Concatenate datasets\n",
    "concat_dataset = concatenate_datasets([dataset1['train'], dataset2['train']])\n",
    "\n",
    "dataset = DatasetDict({'train': concat_dataset})\n",
    "\n",
    "# Split the train split into 90% train, 10% val\n",
    "split_dataset = dataset['train'].train_test_split(test_size = 0.1, seed = 42)\n",
    "print(split_dataset)\n",
    "\n",
    "# Possible issue due to N/A values on combined datasets and also N/A values"
=======
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
   "id": "1961e533-f17d-4b40-b049-1c2d29639442",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
=======
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5679319590844fe9bb624d7f3d6c464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "augmented_meal_planner_dataset.csv:   0%|          | 0.00/1.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahri\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Mahri\\.cache\\huggingface\\hub\\datasets--sridhar52--Augmented_Meal_Planner_data. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c53855ef674575afd119f5dde7c6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['System', 'User', 'Nutritionist', 'text'],\n",
<<<<<<< HEAD
      "        num_rows: 1339\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['System', 'User', 'Nutritionist', 'text'],\n",
      "        num_rows: 149\n",
=======
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['System', 'User', 'Nutritionist', 'text'],\n",
      "        num_rows: 599\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "dataset = load_dataset(\"Tom158/Nutritional-LLama\")\n",
    "\n",
    "# Split the train split into 90% train, 10% val\n",
    "split_dataset = dataset['train'].train_test_split(test_size = 0.1, seed = 42)\n",
    "print(split_dataset)"
=======
    "dataset1 = load_dataset(\"Tom158/Nutritional-LLama\")\n",
    "# Additional dataset\n",
    "dataset2 = load_dataset(\"sridhar52/Augmented_Meal_Planner_data\")\n",
    "\n",
    "# Rename columns\n",
    "dataset2 = dataset2.rename_column('input', 'User')\n",
    "dataset2 = dataset2.rename_column('output', 'Nutritionist')\n",
    "\n",
    "# Concatenate datasets\n",
    "concat_dataset = concatenate_datasets([dataset1['train'], dataset2['train']])\n",
    "\n",
    "dataset = DatasetDict({'train': concat_dataset})\n",
    "\n",
    "# Split the train split into 90% train, 10% val\n",
    "split_dataset = dataset['train'].train_test_split(test_size = 0.1, seed = 42)\n",
    "print(split_dataset)\n",
    "\n",
    "# Possible issue due to N/A values on combined datasets and also N/A values"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "fd367ce9-5691-4491-9e5f-6013dc5162ba",
   "metadata": {},
   "outputs": [],
=======
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
   "id": "fd367ce9-5691-4491-9e5f-6013dc5162ba",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: I've been advised to increase my protein intake while going through treatment for cancer. I love seafood, but I'm not sure if cod is okay with my doctor recommending it as part of my meal plan. What are the benefits of including cod in my diet?\n",
      "A: That's a great choice! As you're looking to boost your protein levels during this challenging time, cod can be an excellent addition to your meals. The high-quality protein content in cod can help support your overall health and muscle mass. Additionally, cod is low in fat and saturated fat, which is perfect for someone with cancer who needs to prioritize a balanced diet. Your doctor might have specific guidelines, but generally speaking, moderate amounts of cod as part of a well-rounded meal plan can be a nutritious choice. Just remember to also focus on fiber-rich foods like fruits and vegetables to help keep your digestive system healthy during treatment.\n",
      "\n",
      "For an added boost, I'd recommend pairing cod with some brown rice or quinoa for a complex carb source, and steamed veggies like broccoli or green beans for a dose of fiber and antioxidants. If you're looking for more seafood options, other types like salmon or tilapia might also be great choices to explore. Just always consult with your doctor before making any significant changes to your diet.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4737f3fe084534b0d842049718fa37",
=======
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc4f1e436dd4ec9a79bb17e05c4f148",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "Map:   0%|          | 0/1339 [00:00<?, ? examples/s]"
=======
       "Map:   0%|          | 0/5389 [00:00<?, ? examples/s]"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "6a7ac4ae20b94ec4b05621e43077abb3",
=======
       "model_id": "2d783abda83a4eb3add3326877d399b3",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "Map:   0%|          | 0/149 [00:00<?, ? examples/s]"
=======
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the best ways to incorporate strips into my diet while taking care of my health, considering I'm going through treatment for cancer?\n",
      "A: Considering your current situation and the nutritional values of strips, it's great that you're thinking about incorporating them into your diet. Since you have a significant amount of fiber from the strips, it can help support healthy digestion. Given your age and weight, it's essential to prioritize nutrient-dense foods to maintain your overall well-being during cancer treatment. While strips may not be packed with protein or fat, they can still provide some carbohydrates for energy. To make the most of this food item, try pairing them with other fiber-rich foods like fruits, vegetables, or whole grains. Additionally, be mindful of your calorie intake and balance it with other nutrient-dense foods to ensure you're getting enough overall nutrition. Remember to also stay hydrated by drinking plenty of water throughout the day.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d5880b82774b549eaabca4b4c799a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dda5021f0dc48c889c8776c8aae2130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
<<<<<<< HEAD
      "        num_rows: 1339\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 149\n",
=======
      "        num_rows: 5389\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 599\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      "    })\n",
      "})\n"
     ]
    }
   ],
>>>>>>> Dev
   "source": [
    "# 1.3 Step 2: Tokenize the Text\n",
    "# Note: We have used distilbert-base-uncased tokenizer in Tutorial_1\n",
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model (GPT-2 Small)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def format_qa(example):\n",
    "    return {\n",
    "        \"prompt\": f\"Q: {example[\"User\"]}\\nA: {example[\"Nutritionist\"]}\"\n",
    "    }\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"prompt\"],\n",
    "        # padding = \"max_length\",\n",
    "        # truncation = True,\n",
    "        # max_length = 128,\n",
    "    )\n",
    "\n",
    "split_dataset = split_dataset.map(format_qa)\n",
    "print(split_dataset[\"train\"][0][\"prompt\"])\n",
    "tokenized_dataset = split_dataset.map(tokenize_function, batched = True, remove_columns = [\"System\", \"User\", \"Nutritionist\", \"text\", \"prompt\"])\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   "id": "5ecd3598-6798-4bf3-b6ba-b1416ec87c64",
   "metadata": {
    "scrolled": true
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "1fe632ad977c4c85ba92dd595e5b0bb5",
=======
       "model_id": "a5c5ebb49edd41ffbef65f0096faf9bf",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "Map:   0%|          | 0/1339 [00:00<?, ? examples/s]"
=======
       "Map:   0%|          | 0/5389 [00:00<?, ? examples/s]"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
<<<<<<< HEAD
       "model_id": "81838938ef454e34939b1e4f44bd7209",
=======
       "model_id": "ee5627ba8c224c8bb72ad06055c5b450",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
<<<<<<< HEAD
       "Map:   0%|          | 0/149 [00:00<?, ? examples/s]"
=======
       "Map:   0%|          | 0/599 [00:00<?, ? examples/s]"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [48,\n",
       "  25,\n",
<<<<<<< HEAD
       "  314,\n",
       "  1053,\n",
       "  587,\n",
       "  13030,\n",
       "  284,\n",
       "  2620,\n",
       "  616,\n",
       "  7532,\n",
       "  10337,\n",
       "  981,\n",
=======
       "  1867,\n",
       "  389,\n",
       "  262,\n",
       "  1266,\n",
       "  2842,\n",
       "  284,\n",
       "  19330,\n",
       "  22670,\n",
       "  656,\n",
       "  616,\n",
       "  5496,\n",
       "  981,\n",
       "  2263,\n",
       "  1337,\n",
       "  286,\n",
       "  616,\n",
       "  1535,\n",
       "  11,\n",
       "  6402,\n",
       "  314,\n",
       "  1101,\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "  1016,\n",
       "  832,\n",
       "  3513,\n",
       "  329,\n",
       "  4890,\n",
<<<<<<< HEAD
       "  13,\n",
       "  314,\n",
       "  1842,\n",
       "  31473,\n",
       "  11,\n",
       "  475,\n",
       "  314,\n",
       "  1101,\n",
       "  407,\n",
       "  1654,\n",
       "  611,\n",
       "  14873,\n",
       "  318,\n",
       "  8788,\n",
       "  351,\n",
       "  616,\n",
       "  6253,\n",
       "  34639,\n",
       "  340,\n",
       "  355,\n",
       "  636,\n",
       "  286,\n",
       "  616,\n",
       "  9799,\n",
       "  1410,\n",
       "  13,\n",
       "  1867,\n",
       "  389,\n",
       "  262,\n",
       "  4034,\n",
       "  286,\n",
       "  1390,\n",
       "  14873,\n",
       "  287,\n",
       "  616,\n",
       "  5496,\n",
=======
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "  30,\n",
       "  198,\n",
       "  32,\n",
       "  25,\n",
<<<<<<< HEAD
       "  1320,\n",
       "  338,\n",
       "  257,\n",
       "  1049,\n",
       "  3572,\n",
       "  0,\n",
       "  1081,\n",
       "  345,\n",
       "  821,\n",
       "  2045,\n",
       "  284,\n",
       "  5750,\n",
       "  534,\n",
       "  7532,\n",
       "  2974,\n",
       "  1141,\n",
       "  428,\n",
       "  9389,\n",
       "  640,\n",
       "  11,\n",
       "  14873,\n",
       "  460,\n",
       "  307,\n",
       "  281,\n",
       "  6275,\n",
       "  3090,\n",
       "  284,\n",
       "  534,\n",
       "  13840,\n",
       "  13,\n",
       "  383,\n",
       "  1029,\n",
       "  12,\n",
       "  13237,\n",
       "  7532,\n",
       "  2695,\n",
       "  287,\n",
       "  14873,\n",
       "  460,\n",
       "  1037,\n",
       "  1104,\n",
       "  534,\n",
       "  4045,\n",
       "  1535,\n",
       "  290,\n",
       "  8280,\n",
       "  2347,\n",
       "  13,\n",
       "  12032,\n",
       "  11,\n",
       "  14873,\n",
       "  318,\n",
       "  1877,\n",
       "  287,\n",
       "  3735,\n",
       "  290,\n",
       "  24725,\n",
       "  3735,\n",
       "  11,\n",
       "  543,\n",
       "  318,\n",
       "  2818,\n",
       "  329,\n",
       "  2130,\n",
       "  351,\n",
       "  4890,\n",
       "  508,\n",
       "  2476,\n",
       "  284,\n",
       "  32980,\n",
       "  257],\n",
=======
       "  27662,\n",
       "  534,\n",
       "  1459,\n",
       "  3074,\n",
       "  290,\n",
       "  262,\n",
       "  24000,\n",
       "  3815,\n",
       "  286,\n",
       "  22670,\n",
       "  11,\n",
       "  340,\n",
       "  338,\n",
       "  1049,\n",
       "  326,\n",
       "  345,\n",
       "  821,\n",
       "  3612,\n",
       "  546,\n",
       "  29927,\n",
       "  606,\n",
       "  656,\n",
       "  534,\n",
       "  5496,\n",
       "  13,\n",
       "  4619,\n",
       "  345,\n",
       "  423,\n",
       "  257,\n",
       "  2383,\n",
       "  2033,\n",
       "  286,\n",
       "  13608,\n",
       "  422,\n",
       "  262,\n",
       "  22670,\n",
       "  11,\n",
       "  340,\n",
       "  460,\n",
       "  1037,\n",
       "  1104,\n",
       "  5448,\n",
       "  44639,\n",
       "  13,\n",
       "  11259,\n",
       "  534,\n",
       "  2479,\n",
       "  290,\n",
       "  3463,\n",
       "  11,\n",
       "  340,\n",
       "  338,\n",
       "  6393,\n",
       "  284,\n",
       "  32980,\n",
       "  27560,\n",
       "  12,\n",
       "  67,\n",
       "  1072,\n",
       "  9013,\n",
       "  284,\n",
       "  5529,\n",
       "  534,\n",
       "  4045,\n",
       "  880,\n",
       "  12,\n",
       "  11873,\n",
       "  1141,\n",
       "  4890,\n",
       "  3513,\n",
       "  13,\n",
       "  2893,\n",
       "  22670,\n",
       "  743,\n",
       "  407,\n",
       "  307,\n",
       "  11856,\n",
       "  351,\n",
       "  7532,\n",
       "  393,\n",
       "  3735,\n",
       "  11,\n",
       "  484,\n",
       "  460,\n",
       "  991,\n",
       "  2148,\n",
       "  617,\n",
       "  32328,\n",
       "  329,\n",
       "  2568,\n",
       "  13,\n",
       "  1675,\n",
       "  787,\n",
       "  262,\n",
       "  749,\n",
       "  286],\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [48,\n",
       "  25,\n",
<<<<<<< HEAD
       "  314,\n",
       "  1053,\n",
       "  587,\n",
       "  13030,\n",
       "  284,\n",
       "  2620,\n",
       "  616,\n",
       "  7532,\n",
       "  10337,\n",
       "  981,\n",
=======
       "  1867,\n",
       "  389,\n",
       "  262,\n",
       "  1266,\n",
       "  2842,\n",
       "  284,\n",
       "  19330,\n",
       "  22670,\n",
       "  656,\n",
       "  616,\n",
       "  5496,\n",
       "  981,\n",
       "  2263,\n",
       "  1337,\n",
       "  286,\n",
       "  616,\n",
       "  1535,\n",
       "  11,\n",
       "  6402,\n",
       "  314,\n",
       "  1101,\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "  1016,\n",
       "  832,\n",
       "  3513,\n",
       "  329,\n",
       "  4890,\n",
<<<<<<< HEAD
       "  13,\n",
       "  314,\n",
       "  1842,\n",
       "  31473,\n",
       "  11,\n",
       "  475,\n",
       "  314,\n",
       "  1101,\n",
       "  407,\n",
       "  1654,\n",
       "  611,\n",
       "  14873,\n",
       "  318,\n",
       "  8788,\n",
       "  351,\n",
       "  616,\n",
       "  6253,\n",
       "  34639,\n",
       "  340,\n",
       "  355,\n",
       "  636,\n",
       "  286,\n",
       "  616,\n",
       "  9799,\n",
       "  1410,\n",
       "  13,\n",
       "  1867,\n",
       "  389,\n",
       "  262,\n",
       "  4034,\n",
       "  286,\n",
       "  1390,\n",
       "  14873,\n",
       "  287,\n",
       "  616,\n",
       "  5496,\n",
=======
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "  30,\n",
       "  198,\n",
       "  32,\n",
       "  25,\n",
<<<<<<< HEAD
       "  1320,\n",
       "  338,\n",
       "  257,\n",
       "  1049,\n",
       "  3572,\n",
       "  0,\n",
       "  1081,\n",
       "  345,\n",
       "  821,\n",
       "  2045,\n",
       "  284,\n",
       "  5750,\n",
       "  534,\n",
       "  7532,\n",
       "  2974,\n",
       "  1141,\n",
       "  428,\n",
       "  9389,\n",
       "  640,\n",
       "  11,\n",
       "  14873,\n",
       "  460,\n",
       "  307,\n",
       "  281,\n",
       "  6275,\n",
       "  3090,\n",
       "  284,\n",
       "  534,\n",
       "  13840,\n",
       "  13,\n",
       "  383,\n",
       "  1029,\n",
       "  12,\n",
       "  13237,\n",
       "  7532,\n",
       "  2695,\n",
       "  287,\n",
       "  14873,\n",
       "  460,\n",
       "  1037,\n",
       "  1104,\n",
       "  534,\n",
       "  4045,\n",
       "  1535,\n",
       "  290,\n",
       "  8280,\n",
       "  2347,\n",
       "  13,\n",
       "  12032,\n",
       "  11,\n",
       "  14873,\n",
       "  318,\n",
       "  1877,\n",
       "  287,\n",
       "  3735,\n",
       "  290,\n",
       "  24725,\n",
       "  3735,\n",
       "  11,\n",
       "  543,\n",
       "  318,\n",
       "  2818,\n",
       "  329,\n",
       "  2130,\n",
       "  351,\n",
       "  4890,\n",
       "  508,\n",
       "  2476,\n",
       "  284,\n",
       "  32980,\n",
       "  257]}"
      ]
     },
     "execution_count": 5,
=======
       "  27662,\n",
       "  534,\n",
       "  1459,\n",
       "  3074,\n",
       "  290,\n",
       "  262,\n",
       "  24000,\n",
       "  3815,\n",
       "  286,\n",
       "  22670,\n",
       "  11,\n",
       "  340,\n",
       "  338,\n",
       "  1049,\n",
       "  326,\n",
       "  345,\n",
       "  821,\n",
       "  3612,\n",
       "  546,\n",
       "  29927,\n",
       "  606,\n",
       "  656,\n",
       "  534,\n",
       "  5496,\n",
       "  13,\n",
       "  4619,\n",
       "  345,\n",
       "  423,\n",
       "  257,\n",
       "  2383,\n",
       "  2033,\n",
       "  286,\n",
       "  13608,\n",
       "  422,\n",
       "  262,\n",
       "  22670,\n",
       "  11,\n",
       "  340,\n",
       "  460,\n",
       "  1037,\n",
       "  1104,\n",
       "  5448,\n",
       "  44639,\n",
       "  13,\n",
       "  11259,\n",
       "  534,\n",
       "  2479,\n",
       "  290,\n",
       "  3463,\n",
       "  11,\n",
       "  340,\n",
       "  338,\n",
       "  6393,\n",
       "  284,\n",
       "  32980,\n",
       "  27560,\n",
       "  12,\n",
       "  67,\n",
       "  1072,\n",
       "  9013,\n",
       "  284,\n",
       "  5529,\n",
       "  534,\n",
       "  4045,\n",
       "  880,\n",
       "  12,\n",
       "  11873,\n",
       "  1141,\n",
       "  4890,\n",
       "  3513,\n",
       "  13,\n",
       "  2893,\n",
       "  22670,\n",
       "  743,\n",
       "  407,\n",
       "  307,\n",
       "  11856,\n",
       "  351,\n",
       "  7532,\n",
       "  393,\n",
       "  3735,\n",
       "  11,\n",
       "  484,\n",
       "  460,\n",
       "  991,\n",
       "  2148,\n",
       "  617,\n",
       "  32328,\n",
       "  329,\n",
       "  2568,\n",
       "  13,\n",
       "  1675,\n",
       "  787,\n",
       "  262,\n",
       "  749,\n",
       "  286]}"
      ]
     },
     "execution_count": 6,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Dev
   "source": [
    "# 1.4 Step 3: Group Tokens for Language Modeling\n",
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = sum(examples['input_ids'], [])\n",
    "    concatenated_attention_mask = sum(examples['attention_mask'], [])\n",
    "    \n",
    "    total_length = (len(concatenated) // block_size) * block_size\n",
    "    result = {\n",
    "        'input_ids': [concatenated[i:i + block_size] for i in range(0, total_length, block_size)],\n",
    "        'attention_mask': [concatenated_attention_mask[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "    }\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(group_texts, batched = True)\n",
    "lm_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   "id": "0c270976-bb3b-426e-970d-272aa55643d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Step 4: Load a Pretrained Model for Language Modeling\n",
<<<<<<< HEAD
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\") # Note: we are using gpt2"
=======
<<<<<<< HEAD
    "# model = AutoModelForCausalLM.from_pretrained(model_name) # Note: we are using gpt2"
=======
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\") # Note: we are using gpt2"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "50b1a3eb-9ba3-4ca6-9e36-077a06d7e345",
   "metadata": {},
   "outputs": [],
=======
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 8,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
   "id": "50b1a3eb-9ba3-4ca6-9e36-077a06d7e345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\tuand\\AppData\\Local\\Temp\\ipykernel_24348\\2639110744.py:68: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
=======
      "C:\\Users\\Mahri\\AppData\\Local\\Temp\\ipykernel_8316\\3712485384.py:100: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
<<<<<<< HEAD
       "      <progress value='1164' max='1164' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1164/1164 11:24, Epoch 3/3]\n",
=======
       "      <progress value='1400' max='3324' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1400/3324 10:26 < 14:22, 2.23 it/s, Epoch 1/3]\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
<<<<<<< HEAD
=======
       "      <th>Validation Loss</th>\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
<<<<<<< HEAD
       "      <td>500</td>\n",
       "      <td>1.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.178100</td>\n",
=======
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.024071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.839486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.762142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.734860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.711419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.706386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.688703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>0.681122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.669165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.658447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.665515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.652144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.650871</td>\n",
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1164, training_loss=1.326998038799902, metrics={'train_runtime': 685.1172, 'train_samples_per_second': 6.787, 'train_steps_per_second': 1.699, 'total_flos': 1079614557388800.0, 'train_loss': 1.326998038799902, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('my_model\\\\tokenizer_config.json',\n",
       " 'my_model\\\\special_tokens_map.json',\n",
       " 'my_model\\\\vocab.json',\n",
       " 'my_model\\\\merges.txt',\n",
       " 'my_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 8,
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Dev
   "source": [
    "# Trains the model and selects a GPU if possible\n",
    "# Some hyperparameters were changed as Google Colab's TPU and GPU\n",
    "# had expired, so the CPU was utilized with parameters adjusted to\n",
    "# allow it to train in a reasonable amount of time.\n",
    "\n",
    "# # 1. Leverage mixed precision training\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir = \"./lm_checkpoints\",\n",
    "    \n",
    "#     # 2. Aligned evaluation and saving strategy\n",
    "#     eval_strategy = \"steps\", # Evaluate at specific steps\n",
    "#     eval_steps = 100, # Evaluate every 100 steps\n",
    "#     save_strategy = \"steps\", # Does not allow equality to \"epoch\"\n",
    "#     save_steps = 100, # Save every 100 steps\n",
    "#     save_total_limit = 2, # Keep only the 2 most recent checkpoints\n",
    "\n",
    "#     # 3. Increase learning rate and use warmup\n",
    "#     learning_rate = 5e-5, # Higher learning rate\n",
    "#     warmup_ratio = 0.1, # Warm up for first 10% of training\n",
    "\n",
    "#     # 4. Increase batch size\n",
    "#     per_device_train_batch_size = 16, # Increase if your GPU has enough memory\n",
    "#     # per_device_eval_batch_size = 16,\n",
    "#     gradient_accumulation_steps = 2, # Simulate larger batch sizes\n",
    "\n",
    "#     # 5. Enable fp16 training (mixed precision)\n",
    "#     fp16 = True, # Enable mixed precision training\n",
    "\n",
    "#     # 6. Early stopping configuration\n",
    "#     load_best_model_at_end = True, # Load the best model when training ends\n",
    "#     metric_for_best_model = \"loss\", # Use evaluation loss as the metric to track\n",
    "#     greater_is_better = False, # Lower loss is better\n",
    "\n",
    "#     # 7. Other optimizations\n",
    "#     weight_decay = 0.01,\n",
    "#     logging_steps = 50, # Less frequent logging\n",
    "#     report_to = \"none\",\n",
    "\n",
    "#     # 8. Enable data parallelism if multiple GPUs are available\n",
    "#     dataloader_num_workers =  4, # Use multiple CPU cores for data loading\n",
    "\n",
    "#     # 9. Set number of epochs\n",
    "#     num_train_epochs = 1, # Maintain the original epochs setting\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./qa-gpt2\",\n",
    "    overwrite_output_dir = True,\n",
    "    \n",
    "    num_train_epochs = 3,\n",
    "    \n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
>>>>>>> Dev
    "\n",
    "    #test\n",
    "\n",
    "    learning_rate = 5e-5, # Higher learning rate\n",
    "    warmup_ratio = 0.1, # Warm up for first 10% of training\n",
    "    eval_strategy=\"steps\", # Evaluate at specific steps\n",
    "    save_strategy=\"steps\", # Save at the same frequency as evaluation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
<<<<<<< HEAD
=======
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
    "    \n",
    "    save_steps = 100,\n",
    "    eval_steps = 100,\n",
    "    save_total_limit = 2,\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
>>>>>>> Dev
    "\n",
    "    # test \n",
    "    load_best_model_at_end = True, # Load the best model when training ends\n",
    "    metric_for_best_model = \"loss\", # Use evaluation loss as the metric to track\n",
    "    greater_is_better = False, # Lower loss is better\n",
    "      \n",
    "    weight_decay = 0.01,\n",
<<<<<<< HEAD
=======
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
    "    \n",
    "    prediction_loss_only = True,\n",
    "    logging_dir = \"./logs\"\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer, mlm = False\n",
    ")\n",
    "\n",
    "# 9. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = lm_dataset[\"train\"],\n",
    "    eval_dataset = lm_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
<<<<<<< HEAD
    "\n",
    "\n",
=======
<<<<<<< HEAD
>>>>>>> Dev
    ")\n",
    "\n",
    "# 10. Optional: Use early stopping\n",
    "early_stopping_callback = transformers.EarlyStoppingCallback(\n",
    "     early_stopping_patience = 3,\n",
    "     early_stopping_threshold = 0.01\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
=======
    "\n",
    "\n",
    ")\n",
    "\n",
    "# 10. Optional: Use early stopping\n",
    "early_stopping_callback = transformers.EarlyStoppingCallback(\n",
    "     early_stopping_patience = 3,\n",
    "     early_stopping_threshold = 0.01\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"my_model\")\n",
    "tokenizer.save_pretrained(\"my_model\")"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "fe34745c-f9e5-4d84-a0dd-6f486c8f74a5",
   "metadata": {},
   "outputs": [],
=======
<<<<<<< HEAD
   "execution_count": 8,
   "id": "fe34745c-f9e5-4d84-a0dd-6f486c8f74a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'joblib' has no attribute 'dumo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdumo(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'joblib' has no attribute 'dumo'"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "fe34745c-f9e5-4d84-a0dd-6f486c8f74a5",
   "metadata": {},
   "outputs": [],
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   "source": [
    "joblib.dump(model, \"model.pkl\")\n",
    "joblib.dump(tokenizer, \"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c67c2-46b1-4d32-acf4-72fa97c11ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"model.pkl\")\n",
    "tokenizer = joblib.load(\"tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "457ed278-cdcd-477b-bcfe-50708f135a48",
   "metadata": {},
   "outputs": [],
=======
<<<<<<< HEAD
   "execution_count": 9,
   "id": "457ed278-cdcd-477b-bcfe-50708f135a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Can you recommend me a healthy diet?\n",
      "A: Absolutely! Wheat flakes are an excellent choice to help you maintain your energy levels and weight. They're packed with fiber and carbohydrates which can help keep you full and satisfied between meals. The protein content in wheat flakes is also impressive, especially with its high\n",
      "Q: I prefer Chinese cuisine, and I want to lower my sodium. What can you recommend me?\n",
      "A: Sorry to hear that you're concerned about your sodium intake! While Cakes are a wonderful treat, it's great that you're mindful of your sodium intake. While an occasional serving is all right, regular consumption might cause a spike in your blood pressure\n",
      "Q: I've been trying to lose some weight and get in shape, but I love ground lean as part of my breakfast routine. Is it okay to include this in my diet considering I'm 43 and 109kg with overweight?\n",
      "A: As you're working on your weight, it's great that you're taking steps towards a healthier lifestyle. Ground lean can be a good source of protein every now and then, but I would recommend being mindful of its high fat content - it can be\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "457ed278-cdcd-477b-bcfe-50708f135a48",
   "metadata": {},
   "outputs": [],
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
   "source": [
    "qa_pipeline = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "output1 = qa_pipeline(\"Q: Can you recommend me a healthy diet?\\nA:\", max_new_tokens = 50)\n",
    "# output2 = qa_pipeline(\"Q: A diet consisting of 100 mg of salt, 200 mg of fat, and\\nA:\", max_new_tokens = 50)\n",
    "output3 = qa_pipeline(\"Q: I prefer Chinese cuisine, and I want to lower my sodium. What can you recommend me?\\nA:\", max_new_tokens = 50)\n",
    "# output4 = qa_pipeline(\"Q: I am a pescatarian and enjoy spicy food. What should I eat?\\nA:\", max_new_tokens = 50)\n",
    "# output5 = qa_pipeline(\"Q: I need to consume more fruit and carbohydrates a day. What should I do?\\nA:\", max_new_tokens = 50)\n",
    "# output6 = qa_pipeline(\"Q: My maximum daily calories is 1,200 so I should consume \\nA:\", max_new_tokens = 50)\n",
    "# output7 = qa_pipeline(\"Q: I am a vegetarian and I enjoy food that is sweet and salty. Can you recommend me a diet?\\nA:\", max_new_tokens = 50)\n",
    "output8 = qa_pipeline(\"Q: I've been trying to lose some weight and get in shape, but I love ground lean as part of my breakfast routine. Is it okay to include this in my diet considering I'm 43 and 109kg with overweight?\\nA:\", max_new_tokens = 50)\n",
    "\n",
    "# 3 unique sentences for generation testing\n",
    "print(output1[0][\"generated_text\"])\n",
    "# print(output2[0][\"generated_text\"])\n",
    "print(output3[0][\"generated_text\"])\n",
    "# print(output4[0][\"generated_text\"])\n",
    "# print(output5[0][\"generated_text\"])\n",
    "# print(output6[0][\"generated_text\"])\n",
    "# print(output7[0][\"generated_text\"])\n",
    "print(output8[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07939dc-a461-4b9c-8d95-b410174d4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generation function\n",
    "def generate_answer(prompt):\n",
    "    try:\n",
    "        result = qa_pipeline(prompt, max_new_tokens = 100, do_sample = True, temperature = 0.9)\n",
    "        result = result[0]['generated_text']\n",
    "\n",
    "        # Extract answer after \"A:\"\n",
    "        if \"A:\" in result:\n",
    "            result = result.split(\"A:\", 1)[1].strip()\n",
    "        else:\n",
    "            result = result.strip()  # fallback if format breaks\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] {str(e)}\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_answer,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Type a question or prompt...\", label=\"Prompt\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"GPT-2 Q&A Generator\",\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e7762a-58a6-4f2d-940c-03e83ccae706",
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
>>>>>>> Dev
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20beac9-0a59-404d-b766-ca1d0048def1",
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
=======
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
>>>>>>> Dev
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
=======
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
>>>>>>> d4cbcb57ca52375a50d02d7186047536cf277813
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
